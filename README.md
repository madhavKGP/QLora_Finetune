# QLoRA Fine-Tuning:

This project demonstrates an advanced, highly efficient methodology for fine-tuning a pre-trained Transformer encoder (e.g., **DeBERTa-v3-small**) to solve a comparative ranking task. It trains a model to predict the winner between two competing language model responses (**Model A vs. Model B**) based on human preference data (e.g., the **LMSys Chatbot Arena** dataset).

The entire pipeline is optimized for memory and speed using **QLoRA** and is structured for robust training on **L40S GPU** using **PyTorch**.

---

## ğŸŒŸ The Core Technology: QLoRA Explained

### 1ï¸âƒ£ Q (Quantization): 4-bit Base Model

The project loads the large pre-trained model in **4-bit precision (NF4 data type)** â€” crucial for memory efficiency.

* **Benefit:** Drastically reduces memory footprint (e.g., from 16 GB â†’ 4 GB).
* **Methodology:** QLoRA uses **NormalFloat4 (NF4)** and **Double Quantization** to shrink model size without significantly sacrificing performance.

### 2ï¸âƒ£ LoRA (Low-Rank Adaptation): Trainable Adapters

LoRA makes fine-tuning feasible by drastically reducing the number of trainable parameters. It introduces a small, parallel pathway for weight updates inside the massive Transformer layers, allowing the model to learn new tasks with minimal overhead.

Instead of training all weights ($W_0$), LoRA introduces a smaller update path ($\Delta W$):

$$
h = W_0 \cdot x \quad \rightarrow \quad h = (W_0 + BA) \cdot x
$$

* $W_0$: Large, original, **frozen** weight matrix.
* $A, B$: Small, **trainable** matrices ($A$ and $B$) that form the low-rank adapter.

| Metric | Value |
| :--- | :--- |
| Trainable Params | $663,552$ |
| All Params | $141,967,872$ |
| **Trainable %** | **$0.4674\%$** |

**Checkpoint Size:** Only the tiny adapter matrices are saved â€” measured in **MB**, not **GB**, 2.5MB in our case.

---

## ğŸ§  Model Architecture: Siamese Ranking (Bi-Encoder)

A custom **Siamese network** is designed for comparing two inputs, ensuring both are processed by the same encoder instance.

### Architecture Diagram (Compact ASCII)

```text
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚             QLoRA-Tuned Transformer Encoder             â”‚
       â”‚           (W_0 is frozen, A/B adapters are trained)       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–²          â–²          â–²             â”‚
               â”‚          â”‚          â”‚             â–¼
Input A: P + Res A â”€â–º Tokenize â”€â–º Model â”€â–º Avg Pool â”€â”
                                                     â”‚
                                                     â”œâ”€â–º CONCATENATE â”€â–º Linear (3 Classes)
                                                     â”‚
Input B: P + Res B â”€â–º Tokenize â”€â–º Model â”€â–º Avg Pool â”€â”˜
```


![Training Curve: Loss and Accuracy over Epochs](download.png)
